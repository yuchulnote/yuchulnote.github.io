---
title : "Covid Face Mask Webcam Classification : Custom CNN model with numpy(VGG6)"
categories :
    - Project
tag :
    - [VGG, Face Mask Detection, CNN]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
published: true
---

# Covid Face Mask Webcam Classification : Custom CNN model with numpy(VGG6)

ë„˜íŒŒì´ë¡œ ì§ì ‘ êµ¬í˜„í•´ì„œ ì§œë³´ëŠ” ë§ˆìŠ¤í¬ ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.
VGG netì— ì˜ê°ì„ ë°›ì•„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•œ VGG6 ëª¨ë¸ì…ë‹ˆë‹¤.

ì „ì²´ ì½”ë“œë¡œ ì§„í–‰ëœ ipynbëŠ” ğŸ‘‰ [Mask_Classification](https://github.com/yuchulnote/Personal_Project/blob/main/2-1/Mask_Classification.ipynb)

Deep Learningì´ ì–´ë–¤ì‹ìœ¼ë¡œ êµ¬í˜„ë˜ëŠ”ì§€ ê³µë¶€í•˜ê¸° ìœ„í•œ ëª©í‘œì˜ ì‘í’ˆì´ì˜€ìŠµë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œ ì§„í–‰í•˜ë©° ê³µë¶€ê°€ ë§ì´ ë˜ì—ˆì§€ë§Œ, ì§ì ‘ì ìœ¼ë¡œ numpyë¥¼ ì´ìš©í•´ í•¨ìˆ˜ë„ êµ¬í˜„í•œ ê²ƒì„ ì‚¬ìš©í•˜ë‹¤ë³´ë‹ˆ ì—ëŸ¬ê°€ ë§ì´ ë‚˜ì™”ê³ , í•´ê²°í•˜ëŠ”ë° êµ‰ì¥íˆ ì• ë¥¼ ë¨¹ì—ˆë˜ ê¸°ì–µì´ ë‚©ë‹ˆë‹¤ ã… ã… 

ë˜í•œ í•™ìŠµê³¼ì •ì—ì„œ í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œë¥¼ ë°œê²¬í•˜ì˜€ê³ , ì‹¤ì œë¡œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ë‹ˆ ë§ˆìŠ¤í¬ë¥¼ ì“´ ê²ƒê³¼ ì“°ì§€ ì•Šì€ ê²ƒì„ êµ¬ë¶„í•˜ì§€ ëª»í•˜ëŠ” ê²°ê³¼ë¥¼ ë§ì´í•˜ì˜€ìŠµë‹ˆë‹¤...

ì˜ˆìƒí•˜ëŠ” ì´ìœ ë¡œëŠ”
1. í•™ìŠµë°ì´í„°ëŸ‰ í˜„ì €íˆ ë¶€ì¡±
2. ë°ì´í„° ë¼ë²¨ë§ì„ ì›-í•« ì¸ì½”ë”© í˜•íƒœë¡œ ì§„í–‰í–ˆì–´ì•¼í•˜ëŠ”ë° ë‹¨ìˆœíˆ ìˆ«ìë¡œ ë§¤ê¸´ ì 

ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹œê°„ì´ ë˜ë©´ ì•ˆë˜ëŠ” ë¶€ë¶„ë“¤ì„ ì°¾ì•„ ì œëŒ€ë¡œ ì‘ë™í•˜ê²Œ ì™„ì„±ì„ í•˜ê³  ì‹¶ë„¤ìš” ã…œã…œ ì¼ë‹¨ í˜¹ì‹œë¼ë„ ì´ ê¸€ì„ ì°¸ê³ í•˜ì‹œëŠ” ë¶„ì€ ì–´ë–¤ì‹ìœ¼ë¡œ CNNì´ êµ¬í˜„ë˜ëŠ”ì§€ ì •ë„ì— ì´ˆì ì„ ë§ì¶°ì„œ ë´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.

ì‘í’ˆ ë°œí‘œë‚ ì—ëŠ” ê¸‰í•˜ê²Œ torchë¥¼ ì‚¬ìš©í•œ ë²„ì „ìœ¼ë¡œ ë°œí‘œí•˜ì˜€ê³ , ì´ë§ˆì €ë„ í•™ìŠµë°ì´í„°ê°€ ë¶€ì¡±í•´ì„œì¸ì§€ ì›¹ìº  ì½”ë“œë¥¼ ì²˜ìŒ ì¨ë´ì„œ, ëª¨ë¸ì´ predictí•˜ëŠ” ì‹œê°„ë³´ë‹¤ ë¹ ë¥´ê²Œ ì›¹ìº  ë°ì´í„°ê°€ ë“¤ì–´ê°€ì„œì¸ì§€ ëª¨ë¥´ê² ì§€ë§Œ ì •ë§ ê°€ë”ê°€ë‹¤ ëª‡ë²ˆì”©ë§Œ ì‘ë™ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

(ë°œí‘œë‚ ì— googleì´ë¼ ì €ì¥í•œ ê²ƒì„ ì‚¬ìš©í•œ ê²ƒì„ ëŒë ¸ëŠ”ì§€, torchë¼ê³  ì €ì¥í•œ ê²ƒì„ ì‚¬ìš©í•œ ê²ƒì„ ëŒë ¸ëŠ”ì§€ ì˜ ê¸°ì–µì´ ë‚˜ì§€ ì•Šì•„ì„œ ë‘ê°€ì§€ ì½”ë“œë¥¼ ëª¨ë‘ ê³µìœ í•©ë‹ˆë‹¤. googleì´ë¼ ëª…ì‹œí•œ ê²ƒì€ ê¸‰í•˜ê¸° êµ¬ê¸€ë§ í•œ ê²ƒì—ì„œ ë”°ì™€ì„œ ê·¸ë ‡ê²Œ ì§€ì—ˆìŠµë‹ˆë‹¤ã…‹ã…‹)

[torchë²„ì ¼ í™•ì¸í•˜ê¸°ğŸ˜¿](https://github.com/yuchulnote/Personal_Project/blob/main/2-1/Mask_torch.ipynb)

[googleë²„ì ¼ í™•ì¸í•˜ğŸ˜¹](https://github.com/yuchulnote/Personal_Project/blob/main/2-1/Mask_google.ipynb)

<br>

---

## í•„ìš”í•œ ëª¨ë“ˆ import

```py
import numpy as np
import torchvision
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import glob
import sys, os
sys.path.append(os.pardir) # ë¶€ëª¨ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ì„¤ì •
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
from collections import OrderedDict
import pickle
import torch
%matplotlib inline
import cv2
from imutils.video import VideoStream

print('GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {}'.format(torch.cuda.is_available()))
device = "cuda" if torch.cuda.is_available() else "CPU"
```
---

## ë””ë ‰í† ë¦¬ ì„¤ì •

```py
from pathlib import Path

folder = "Anaconda/Baram"
project_dir = "MaskClassificaion"

base_path = Path("/Users/yuchul/")
project_path = base_path / folder / project_dir
os.chdir(project_path)
for x in list(project_path.glob("*")):
    if x.is_dir():
        dir_name = str(x.relative_to(project_path))
        os.rename(dir_name, dir_name.split(" ", 1)[0])
print(f"í˜„ì¬ ë””ë ‰í† ë¦¬ ìœ„ì¹˜: {os.getcwd()}")
```

```py
current_path = Path().absolute()
data_path = current_path / "data_old"
```

```py
print("í˜„ì¬ ë””ë ‰í† ë¦¬ ìœ„ì¹˜: {}".format(current_path))
```

```py
if (data_path / "mask_cnn").exists():
    print("ì´ë¯¸ 'data/mask_cnn' í´ë”ê°€ ìˆìŠµë‹ˆë‹¤! ì´ì–´ì„œ ì§„í–‰í•˜ì„¸ìš”~")
else: print("ì—†ìŠµë‹ˆë‹¤")
```

```py
data_dir = './data_old/mask_cnn'
```

```py
print(data_path)
```

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •

```py
batch_size = 32
num_epochs = 30
learning_rate = 0.0001
```

---

## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ë¼ë²¨ë§

```py
class MaskNonMaskDataset(Dataset):
    def __init__(self, data_dir, mode=None, transform=None, train_size=None):
        self.all_data = sorted(glob.glob(os.path.join(data_dir, mode, '*', '*')))
        
        #RGB ê°€ ì•„ë‹Œ íŒŒì¼í•„í„°ë§
#         self.all_data = [i for i in self.old_all_data if transform(Image.open(i)).shape[0] == 3]
    
        self.mode = mode
        self.transform = transform
        self.train_size = train_size
    
    def __getitem__(self, index):
        
        data_path = self.all_data[index]
        img = Image.open(data_path)
        if self.transform != None:
            img = self.transform(img)
        
        if os.path.basename(data_path).startswith("Mask"):
            label = 0
        else:
            label = 1
            
        return img, label # [1, 0, 0] , [0, 1, 0], [0, 0, 1]
    
    def __len__(self):
        length = len(self.all_data)
        return length
```

## ë°ì´í„° ì „ì²˜ë¦¬

```py
data_transforms = {
    'train' : transforms.Compose([
        transforms.RandomRotation(5),
        transforms.RandomHorizontalFlip(),
        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),
        transforms.ToTensor(),
        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val' : transforms.Compose([
        transforms.Resize([224, 224]),
        transforms.ToTensor(),
        # transforms.Normalize([0.485,  0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

# train_data = MaskNonMaskDataset(data_dir='./data/mask_cnn', mode='Train', transform=data_transforms['train'])
# val_data = MaskNonMaskDataset(data_dir='./data/mask_cnn', mode='Validation', transform=data_transforms['val'])
# test_data = MaskNonMaskDataset(data_dir='./data/mask_cnn', mode='Test', transform=data_transforms['val'])

# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)
# val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)
# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)
```

ë„˜íŒŒì´ë¡œ êµ¬í˜„í•˜ë©° ì§„í–‰í•˜ë‹¤ë³´ë‹ˆ, Normalizeí•  ë•Œ ì—ëŸ¬ê°€ ë‚˜ì„œ, í•´ê²°í•˜ê¸°ê°€ ì–´ë ¤ì›Œì„œ ì£¼ì„ì²˜ë¦¬í•˜ê³  ì§„í–‰í–ˆì—ˆìŠµë‹ˆë‹¤! ã… ã… 

```py
train_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/Train', transform=data_transforms['train'])
val_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/Validation', transform=data_transforms['val'])
test_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/Test', transform=data_transforms['val'])

add_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/add_data/Train_', transform=None)
```

ë¶€ì¡±í•œ í›ˆë ¨ë°ì´í„°ë¥¼ ë‚˜ì¤‘ì— ì¶”ê°€í•˜ì˜€ì§€ë§Œ, ê·¸ë˜ë„ ë¶€ì¡±í•œ ë°ì´í„° ì–‘ì´ì˜€ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

```py
add_data = torchvision.datasets.ImageFolder(root='./data_old/mask_cnn/add_data/Train', transform=data_transforms['train'])
```

```py
print(len(train_data))
print(len(val_data))
print(len(test_data))
x_train = []
t_train = []
```
ì•„ë§ˆ ipynb ë¥¼ ë³´ì‹œë©´ train ë°ì´í„° ê°œìˆ˜ê°€ 18238ê°œë¡œ ë‚˜ì™€ìˆì„ê²ë‹ˆë‹¤. ì´ëŠ” ë°œí‘œí›„ì— ì¶”ê°€ë¡œ ë„£ì–´ì„œ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚œ ê²ƒì´ì§€ ì‹¤ì œ í•™ìŠµì‹œì¼°ì„ ë‹¹ì‹œì—ëŠ” ì•„ë§ˆ 600~1000ê°œ ì •ë„ ì˜€ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤! 2ë‹¬ ì •ë„ ì§€ë‚˜ê³  ì •ë¦¬í•˜ë ¤ë‹ˆê¹Œ ê¸°ì–µì´ ê°€ë¬¼ê°€ë¬¼í•˜ë„¤ìš” ã… ã… 

```py
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)
```

---

## ê°ì¢… í•„ìš” í•¨ìˆ˜ë“¤ êµ¬í˜„

### ReLU êµ¬í˜„

```py
class ReLU:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0)
        out = x.copy()
        out[self.mask] = 0
        
        return out

    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout

        return dx
```

### grad êµ¬í•˜ëŠ” í•¨ìˆ˜

```py
def get_grad(f, x):
    h = 1e-4 #0.0001
    grad = np.zeros_like(x)
    
    it = np.nditer(x, flag=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x) #f(x+h)
        
        x[idx] = tmp_val - h
        fxh2 = f(x) #f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2*h)
        
        x[idx] = tmp_val # ê°’ ë³µì›
        it.iternext()
        
    return grad
```
### Adam Optimizer êµ¬í˜„

```py
class Adam:
    
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
    
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        
        self.iter += 1
        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)
        
        for key in params.keys():
#             print("ê³„ì‚° ì „ m[key]: ", self.m[key].shape)
#             print("ê³„ì‚° ì „ grads[key]: ", grads[key].shape)
            self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
#             print("key: ", key)
#             print("mí‚¤ê°’", self.m[key].shape)
#             print("gradsí‚¤ê°’", grads[key].shape)
#             print("ví‚¤ê°’", self.v[key].shape)
                  
#             self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
#             self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
            
#             print("paramsí‚¤ê°’", params[key].shape)
            
            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias
            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias
            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)
```

ì¤‘ê°„ ì¤‘ê°„ì— printëŠ” ë””ë²„ê¹…í•  ë•Œ ì‚¬ìš©í–ˆë˜ ë¶€ë¶„ë“¤ì´ë¼ ì£¼ì„ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

### FC_Layer êµ¬í˜„

```py
class FC_Layer:
    
    def __init__(self, W, b):
        self.W = W
        self.b = b
        
        self.x = None
        self.original_x_shape = None
        #ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ë§¤ê°œë³€ìˆ˜ ë¯¸ë¶„
        self.dW = None
        self.db = None
        
    def forward(self, x):
        #í…ì„œ ëŒ€ì‘
#         print("FCì…ë ¥", x.shape)
#         print("ê°€ì¤‘ì¹˜", self.W.shape)
        self.original_x_shape = x.shape
        x = x.reshape(x.shape[0], -1)
        self.x = x
#         print("FC reshape ì…ë ¥", x.shape)
        out = np.dot(self.x, self.W) + self.b
#         print("FCì¶œë ¥", out.shape)
        return out
    
    def backward(self, dout):
#         print("FC backì…ë ¥", dout.shape)
        dx = np.dot(dout, self.W.T) #.TëŠ” Transpose
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        
        dx = dx.reshape(*self.original_x_shape) # ì…ë ¥ ë°ì´í„° ëª¨ì–‘ ë³€ê²½(í…ì„œ ëŒ€ì‘)
#         print("FC backì¶œë ¥", dx.shape)
        return dx
```

### Dropout êµ¬í˜„

```py
class Dropout:
    
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None
    
    def forward(self, x, train_flg=True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio 
            # xëª¨ì–‘ì— ë§ì¶°ì„œ ëœë¤í•œ í–‰ë ¬ìƒì„±, ì›ì†Œë³„ 0~1 ì‚¬ì´ì˜ ëœë˜í•œ ê°’, 
            #ê·¸ ê°’ì´ dropout_ratioë³´ë‹¤ í° ê°’ì´ true, ì‘ìœ¼ë©´ falseì¸ í–‰ë ¬ì„ maskì— ì €ì¥
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)
        
    def backward(self, dout):
        return dout * self.mask
```

### im2col & col2im
im2col & col2im ì€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œ ê°„ë‹¨íˆ ì»¨ë³¼ë£¨ì…˜ì—°ì‚°ì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ numpyë¡œ êµ¬í˜„í•˜ê¸°ìœ„í•´ í‰íƒ„í™”í•˜ê³  ë˜ëŒë¦¬ê¸°ìœ„í•œ í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤.

```py
def im2col(input_data, filter_h, filter_w, stride=1, pad=0):
    """ë‹¤ìˆ˜ì˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜í•œë‹¤(í‰íƒ„í™”).
    
    Parameters
    ----------
    input_data : 4ì°¨ì› ë°°ì—´ í˜•íƒœì˜ ì…ë ¥ ë°ì´í„°(ì´ë¯¸ì§€ ìˆ˜, ì±„ë„ ìˆ˜, ë†’ì´, ë„ˆë¹„)
    filter_h : í•„í„°ì˜ ë†’ì´
    filter_w : í•„í„°ì˜ ë„ˆë¹„
    stride : ìŠ¤íŠ¸ë¼ì´ë“œ
    pad : íŒ¨ë”©
    
    Returns
    -------
    col : 2ì°¨ì› ë°°ì—´
    """
    N, C, H, W = input_data.shape
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1

    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')
    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))

    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]

    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)
    return col
```

```py
def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):
    """(im2colê³¼ ë°˜ëŒ€) 2ì°¨ì› ë°°ì—´ì„ ì…ë ¥ë°›ì•„ ë‹¤ìˆ˜ì˜ ì´ë¯¸ì§€ ë¬¶ìŒìœ¼ë¡œ ë³€í™˜í•œë‹¤.
    
    Parameters
    ----------
    col : 2ì°¨ì› ë°°ì—´(ì…ë ¥ ë°ì´í„°)
    input_shape : ì›ë˜ ì´ë¯¸ì§€ ë°ì´í„°ì˜ í˜•ìƒï¼ˆì˜ˆï¼š(10, 1, 28, 28)ï¼‰
    filter_h : í•„í„°ì˜ ë†’ì´
    filter_w : í•„í„°ì˜ ë„ˆë¹„
    stride : ìŠ¤íŠ¸ë¼ì´ë“œ
    pad : íŒ¨ë”©
    
    Returns
    -------
    img : ë³€í™˜ëœ ì´ë¯¸ì§€ë“¤
    """
    # print(pad)
    N, C, H, W = input_shape
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1
    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)

    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))
    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]

    return img[:, :, pad:H + pad, pad:W + pad]
```

### Shuffle Dataset

```py
def shuffle_dataset(x, t):
    """ë°ì´í„°ì…‹ì„ ë’¤ì„ëŠ”ë‹¤.
    Parameters
    ----------
    x : í›ˆë ¨ ë°ì´í„°
    t : ì •ë‹µ ë ˆì´ë¸”
    
    Returns
    -------
    x, t : ë’¤ì„ì€ í›ˆë ¨ ë°ì´í„°ì™€ ì •ë‹µ ë ˆì´ë¸”
    """
    permutation = np.random.permutation(x.shape[0])
    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]
    t = t[permutation]

    return x, t
```

### BatchNormalization êµ¬í˜„

```py
class BatchNormalization:
    
    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None # í•©ì„±ê³± ê³„ì¸µì€ 4ì°¨ì›, ì™„ì „ì—°ê²° ê³„ì¸µì€ 2ì°¨ì›  

        # ì‹œí—˜í•  ë•Œ ì‚¬ìš©í•  í‰ê· ê³¼ ë¶„ì‚°
        self.running_mean = running_mean
        self.running_var = running_var  
        
        # backward ì‹œì— ì‚¬ìš©í•  ì¤‘ê°„ ë°ì´í„°
        self.batch_size = None
        self.xc = None
        self.std = None
        self.dgamma = None
        self.dbeta = None

    def forward(self, x, train_flg=True):
#         print("BN ì…ë ¥", x.shape)
        self.input_shape = x.shape
        if x.ndim != 2:
            N, C, H, W = x.shape
            x = x.reshape(N, -1)

        out = self.__forward(x, train_flg)
#         print("BN ì¶œë ¥", out.reshape(*self.input_shape).shape)       
        return out.reshape(*self.input_shape)
            
    def __forward(self, x, train_flg):
        if self.running_mean is None:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)
                        
        if train_flg:
            mu = x.mean(axis=0)
            xc = x - mu
            var = np.mean(xc**2, axis=0)
            std = np.sqrt(var + 10e-7)
            xn = xc / std
            
            self.batch_size = x.shape[0]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            
        else:
            xc = x - self.running_mean
            xn = xc / ((np.sqrt(self.running_var + 10e-7)))
            
        out = self.gamma * xn + self.beta 
        return out

    def backward(self, dout):
#         print('BN backì…ë ¥', dout.shape)
        if dout.ndim != 2:
            N, C, H, W = dout.shape
            dout = dout.reshape(N, -1)

        dx = self.__backward(dout)
#         print('BN backì¶œë ¥ reshapeì „', dx.shape)
        
        dx = dx.reshape(*self.input_shape)
#         print('BN backì¶œë ¥ reshapeí›„', dx.shape)
        return dx

    def __backward(self, dout):
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xn * dout, axis=0)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)
        dvar = 0.5 * dstd / self.std
        dxc += (2.0 / self.batch_size) * self.xc * dvar
        dmu = np.sum(dxc, axis=0)
        dx = dxc - dmu / self.batch_size
        
        self.dgamma = dgamma
        self.dbeta = dbeta
        
        return dx
```

### Convolution êµ¬í˜„

```py
class Convolution:
    def __init__(self, W, b, stride=None, pad=None):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
        
        # ì¤‘ê°„ ë°ì´í„°ï¼ˆbackward ì‹œ ì‚¬ìš©ï¼‰
        self.x = None   
        self.col = None
        self.col_W = None
        
        # ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°
        self.dW = None
        self.db = None

    def forward(self, x):
#         print("Conì…ë ¥ê°’=", x.shape)
#         print("ê°€ì¤‘ì¹˜", self.W.shape)
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)
        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)
#         print("out_h=", out_h)
#         print("out_w=", out_w)
        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T

        out = np.dot(col, col_W) + self.b
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        self.x = x
        self.col = col
        self.col_W = col_W
#         print("Conì¶œë ¥ê°’", out.shape)
        return out

    def backward(self, dout):
#         print("Con backì…ë ¥", dout.shape)
        FN, C, FH, FW = self.W.shape
        dout = dout.transpose(0,2,3,1).reshape(-1, FN)

        self.db = np.sum(dout, axis=0)
        self.dW = np.dot(self.col.T, dout)
        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)

        dcol = np.dot(dout, self.col_W.T)
        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)
#         print("Con backì¶œë ¥", dx.shape)
        return dx
```

### Pooling êµ¬í˜„ (MaxPooling)

```py
class Pooling:
    def __init__(self, pool_h, pool_w, stride, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
        
        self.x = None
        self.arg_max = None

    def forward(self, x):
#         print("Pool ì…ë ¥", x.shape)
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)

        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)

        arg_max = np.argmax(col, axis=1)
        out = np.max(col, axis=1)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)

        self.x = x
        self.arg_max = arg_max
#         print("Pool ì¶œë ¥", out.shape)  
        return out

    def backward(self, dout):
#         print("Pool backì…ë ¥", dout.shape)
        dout = dout.transpose(0, 2, 3, 1)
        
        pool_size = self.pool_h * self.pool_w
        dmax = np.zeros((dout.size, pool_size))
        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()
        dmax = dmax.reshape(dout.shape + (pool_size,)) 
        
        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)
        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)
#         print("Pool backì¶œë ¥", dx.shape)
        return dx
```

### Softmax êµ¬í˜„

```py
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # ì˜¤ë²„í”Œë¡œ ëŒ€ì±…
    return np.exp(x) / np.sum(np.exp(x))
```

### Cross Entropy Loss êµ¬í˜„

```py
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    # í›ˆë ¨ ë°ì´í„°ê°€ ì›-í•« ë²¡í„°ë¼ë©´ ì •ë‹µ ë ˆì´ë¸”ì˜ ì¸ë±ìŠ¤ë¡œ ë°˜í™˜
    if t.size == y.size:
        t = t.argmax(axis=1)
    
    
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

### ìœ„ì— êµ¬í˜„í•œ Softmax, CrossEntropyLossë¡œ Loss êµ¬í•˜ê¸°

```py
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None #ì†ì‹¤í•¨ìˆ˜
        self.y = None #softmaxì˜ ì¶œë ¥
        self.t = None #ì •ë‹µ ë ˆì´ë¸”(ì›-í•« ì¸ì½”ë”© í˜•íƒœ)
    
    def forward(self, x, t):
        
#         print("SoftmaxWithLossì˜ t", t.shape)
        
        self.t = t
        self.y = softmax(x)
        
#         print("Softmax(x) = y: ", self.y.shape)

        self.loss = cross_entropy_error(self.y, self.t)
        
#         print("í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ì—ëŸ¬í•¨ìˆ˜", self.loss.shape)
#         print("í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼ì—ëŸ¬í•¨ìˆ˜", self.loss)
        
        return self.loss
    
    def backward(self, dout=1):
        
        batch_size = self.t.shape[0]
        
#         print("SoftmaxWithLoss backì˜ batchsize", batch_size)

#         print("SoftmaxWithLoss backì˜ yëª¨ì–‘", self.y.shape)
#         print("SoftmaxWithLoss backì˜ tëª¨ì–‘", self.t.shape)
#         print("SoftmaxWithLoss backì˜ y", self.y)
#         print("SoftmaxWithLoss backì˜ t", self.t)
        
        if self.t.size == self.y.size: #ì •ë‹µ ë ˆì´ë¸”ì´ ì›-í•« ì¸ì½”ë”© í˜•íƒœì¼ ë•Œ
            dx = (self.y - self.t) / batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
        
#         print("SoftmaxWithLoss backì˜ dx", dx.shape)
        return dx
```

---

## Network(custom model)

VGG6
êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. Conv-BN-Relu-Conv-BN-Relu-Pool ì´ ë‘ë²ˆ ë°˜ë³µë©ë‹ˆë‹¤. ê·¸ í›„ FC_layer 2ì¸µì„ í†µê³¼í•˜ì—¬ ë§ˆìŠ¤í¬ ì¼ëŠ”ì§€ ì•ˆì¼ëŠ”ì§€ë¥¼ êµ¬ë¶„í•´ì•¼í•˜ë¯€ë¡œ ì¶œë ¥ê°’ì€ 2ì…ë‹ˆë‹¤.

```py
# import numba
class VGG6:
    
    def __init__(self, input_dim={3, 224, 224},
                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':2},
                 conv_param_2 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},
                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':2},
                 conv_param_4 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':2},
                 hidden_size=50, output_size=2):
        
        self.first_flg = True
        
        # ======= ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” =======
        
        pre_node_nums = np.array([3*3*3, 16*3*3, 32*3*3, 32*3*3, 64*7*7, hidden_size])
        weight_init_scales = np.sqrt(2.0 / pre_node_nums) # ReLU ì‚¬ìš©í•  ë•Œ ê¶Œì¥ ì´ˆê¹ƒê°’
        
        self.params = {}
        pre_channel_num = 3
        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4]):
            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])
            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])
            pre_channel_num = conv_param['filter_num']
            self.params['gamma' + str(idx+1)] = 1.0
            self.params['beta' + str(idx+1)] = 0.0
            
        self.params['W5'] = weight_init_scales[4] * np.random.randn(64*7*7, hidden_size)
        self.params['b5'] = np.zeros(hidden_size)
        self.params['W6'] = weight_init_scales[5] * np.random.randn(hidden_size, output_size)
        self.params['b6'] = np.zeros(output_size)
        
        # ======= ê³„ì¸µ ìƒì„± =======
        
        self.layers = []
        
        self.layers.append(Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])) # 0
        self.layers.append(BatchNormalization(self.params['gamma1'], self.params['beta1'])) #1
        self.layers.append(ReLU())
        
        self.layers.append(Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad'])) # 3
        self.layers.append(BatchNormalization(self.params['gamma2'], self.params['beta2'])) #4
        self.layers.append(ReLU())
        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))
        
        self.layers.append(Convolution(self.params['W3'], self.params['b3'], conv_param_3['stride'], conv_param_3['pad'])) # 7
        self.layers.append(BatchNormalization(self.params['gamma3'], self.params['beta3'])) #8
        self.layers.append(ReLU())
        
        self.layers.append(Convolution(self.params['W4'], self.params['b4'], conv_param_4['stride'], conv_param_4['pad'])) # 10
        self.layers.append(BatchNormalization(self.params['gamma4'], self.params['beta4']))#11
        self.layers.append(ReLU())
        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))
        
        self.layers.append(FC_Layer(self.params['W5'], self.params['b5'])) # 14
#         self.layers.append(ReLU())
#         self.layers.append(Dropout(0.5))
        self.layers.append(FC_Layer(self.params['W6'], self.params['b6'])) # 17
#         self.layers.append(ReLU())
#         self.layers.append(Dropout(0.5))
        
#         self.layers.append(SoftmaxWithLoss())
        
        self.last_layer = SoftmaxWithLoss()
        
    def predict(self, x, train_flg=False, first_flg=None):
        
        for layer in self.layers:
            if isinstance(layer, Dropout):
                if self.first_flg and isinstance(layer, BatchNormalization):
                    x = layer.__forward(x, train_flg)
                else:
                    x = layer.forward(x, train_flg)
            
            else:
                x = layer.forward(x)
                     
        return x
    
    def loss(self, x, t):
        y = self.predict(x, train_flg=True, first_flg=self.first_flg)
        self.first_flg = False
        return self.last_layer.forward(y, t)
    
    def accuracy(self, x, t, batch_size=32):
        if t.ndim != 1 : t = np.argmax(t, axis=1)

        acc = 0.0

        for i in range(int(x.shape[0] / batch_size)):
            tx = x[i*batch_size:(i+1)*batch_size]
            tt = t[i*batch_size:(i+1)*batch_size]
            y = self.predict(tx, train_flg=False)
            y = np.argmax(y, axis=1)
            acc += np.sum(y == tt)

        return acc / x.shape[0]
    
#     def accuracy(self, inputs, labels):
        
#         # 0, 1 -> [1, 0], [0, 1]
#          for i in range()
        
#         if t.ndim != 1 : t = np.argmax(t, axis=1)
            
#         acc = 0.0
        
#         for i in range(int(x.shape[0] / batch_size)):
#             tx = x[i*batch_size:(i+1)*batch_size]
#             tt = t[i*batch_size:(i+1)*batch_size]
#             y = self.predict(tx, train_flg=False)
#             y = np.argmax(y, axis=1)
#             acc += np.sum(y == tt)
        
#         return acc / x.shape[0]
    
    def gradient(self, x, t):
        
        #forward
        self.loss(x, t)
        
        #backward
        dout = 1
        dout = self.last_layer.backward(dout)
                
        tmp_layers = self.layers.copy()
        tmp_layers.reverse()
        for layer in tmp_layers:
            dout = layer.backward(dout)

        #ê²°ê³¼ ì €ì¥
        grads = {}
        idx = 0
        
        for i, layer in enumerate(self.layers):
            
            if isinstance(layer, Convolution) or isinstance(layer, FC_Layer):

                grads['W' + str(idx+1)] = self.layers[i].dW
                grads['b' + str(idx+1)] = self.layers[i].db
                
                if(i==14):
                    idx += 1
                            
            elif isinstance(layer, BatchNormalization):
                grads['gamma'+str(idx+1)] = self.layers[i].dgamma
                grads['beta'+str(idx+1)] = self.layers[i].dbeta
                idx += 1
                
        return grads
                    
        
#         for i, layer_idx in enumerate((0, 1, 3, 4, 7, 8, 10, 11, 14, 17)):
#             # print("len(self.layers):",len(self.layers))
#             grads['W' + str(i+1)] = self.layers[layer_idx].dW
#             grads['b' + str(i+1)] = self.layers[layer_idx].db
#             grads['gamma'+str(i+1)] = self.layers[layer_idx].dgamma
#             grads['beta'+str(i+1)] = self.layers[layer_idx].dbeta
                
#         return grads
    
    def save_params(self, file_name='params.pkl'):
        params = {}
        for key, val in self.params.items():
            params[key] = val
        with open(file_name, 'wb') as f:
            pickle.dump(params, f)
        print("training network params:", self.params.keys())
        
        
    def load_params(self, file_name='epoch.pkl'):
        with open(file_name, 'rb') as f:
            params = pickle.load(f)
        for key, val in params.items():
            self.params[key] = val
        
#         for i, layer_idx in enumerate((0, 1, 3, 4, 7, 8, 10, 11, 14, 17)):
#             self.layers[layer_idx].W = self.params['W' + str(i+1)]
#             self.layers[layer_idx].b = self.params['b' + str(i+1)]
        
        idx = 0
        
        for i, layer in enumerate(self.layers):
            if isinstance(layer, Convolution) or isinstance(layer, FC_Layer):

                self.layers[i].W = self.params['W' + str(idx+1)]
                self.layers[i].b = self.params['b' + str(idx+1)]
                
                if(i==14):
                    idx += 1
                            
            elif isinstance(layer, BatchNormalization):
                self.layers[i].gamma = self.params['gamma'+str(idx+1)]
                self.layers[i].beta = self.params['beta'+str(idx+1)]
                idx += 1

        print("Params is successfully loaded!:", self.params.keys())
```

```py
network = VGG6()
```

### Epochë‹¹ Loss ê·¸ë˜í”„ ê·¸ë¦¬ê¸°

```py
def graph(data_list, title, color, save_path):
    batch_num_list = [i for i in range(0, len(data_list))]

    plt.figure(figsize=(20, 10))
    plt.rc('font', size=25)
    plt.plot(batch_num_list, data_list, color=color, marker='o', linestyle='solid')
    plt.title(title)
    plt.xlabel('Epoch')
    
    title = plt.ylabel(title)

    plt.savefig(save_path, dpi=600)
    # plt.show()
    plt.close()
```

### Train í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” í•¨ìˆ˜

```py
class Trainer:
    
    """ì‹ ê²½ë§ í›ˆë ¨ì„ ëŒ€ì‹  í•´ì£¼ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, network, x_train_loader, x_test_loader,
                 epochs=30, mini_batch_size=32,
                 optimizer='adagrad', optimizer_param={'lr':0.0001}, 
                 evaluate_sample_num_per_epoch=None, verbose=True):
        self.network = network
        # print(f"Network Spec: {self.network.keys()}")
        self.verbose = verbose
        self.train_loader = x_train_loader
        self.test_loader = x_test_loader
        # print(x_train[0][0][0][0])
#         self.x_train = x_train
#         self.t_train = t_train
#         self.x_test = x_test
#         self.t_test = t_test
#         self.epochs = epochs
        self.batch_size = mini_batch_size
        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch

        # optimzer
        optimizer_class_dict = {'adam':Adam}
        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)
        
        self.train_size = len(x_train) # x_train.shape[0]
        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)
        self.epochs = epochs
        self.current_iter = 0
        self.current_epoch = 0
        
        self.train_loss_list = []
        self.train_acc_list = []
        self.test_acc_list = []
        self.train_mode = True

    def train_step(self):
#         batch_mask = np.random.choice(self.train_size, self.batch_size).tolist()
        
#         x_batch = []
#         t_batch = []

#         for idx in batch_mask:
#             x_batch.append(self.x_train[idx])
#             t_batch.append(self.t_train[idx])
            
        dataloader = self.train_loader if self.train_mode else self.test_loader
        name = "train" if self.train_mode else "evaluate"

        for x_batch, t_batch in dataloader:
#             print(x_batch.shape)
            grads = self.network.gradient(x_batch, t_batch)
            
#             for key in grads.keys():
#                 print(key, ":", grads[key].shape)
            self.optimizer.update(self.network.params, grads)
            loss = self.network.loss(x_batch, t_batch)
            
            if self.train_mode:
                self.train_loss_list.append(loss)
                
            if self.verbose: print(f"\t{name} loss: {loss}")

            if self.current_iter % self.iter_per_epoch == 0:
                self.current_epoch += 1
    
                x_train_sample, t_train_sample = x_batch, t_batch
                train_acc = self.network.accuracy(x_train_sample, t_train_sample)
            
            if self.train_mode:
                self.train_acc_list.append(train_acc)
            else:
                self.test_acc_list.append(test_acc)

                if self.verbose: print("=== epoch:" + str(self.current_epoch) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc) + " ===")
            self.current_iter += 1
            

    def train(self, current_epochs):
        for epoch in range(self.epochs):
            self.train_step()
            self.network.save_params(file_name=f"epoch_{epoch+current_epochs+1}.pkl")
            print(f"model({epoch+1}/{self.epochs}) is saved!")
            graph(self.train_loss_list, 'loss', 'red', f"epoch_{epoch+current_epochs+1}")

    def test(self, test_data_length = len(test_data)):
        hits = 0.0
        acc = 0.0
        for imgs, labels in self.test_loader:
            hits += self.network.accuracy(imgs, labels)
        acc = hist/test_data_length

        if self.verbose:
            print("=============== Final Test Accuracy ===============")
            print("test acc:" + str(test_acc))
```

```py
trainer = Trainer(network, x_train_loader = train_loader, x_test_loader = test_loader,
                 epochs=15, mini_batch_size=32,
                 optimizer='Adam', optimizer_param={'lr':0.0001},
                 evaluate_sample_num_per_epoch=3)

# trainer.network.load_params(file_name='epoch_1.pkl')

trainer.train(0)

network.save_params("VGG6.pkl")
print("Saved Newwork Parameters!")
```

```py
network.load_params('epoch_15.pkl')

img = cv2.imread('/Users/yuchul/Anaconda/Baram/MaskClassificaion/data_old/mask_cnn/Train/Non_Mask/586_1.jpg')
# img = cv2.imread('/Users/yuchul/Anaconda/Baram/MaskClassificaion/data_old/mask_cnn/Train/Mask/0_0.jpg')
img_resize = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_LINEAR)
fix_img = cv2.cvtColor(img_resize, cv2.COLOR_BGR2RGB)
fimg = fix_img.transpose(2,0,1)

fixed_img = np.expand_dims(fimg, axis=0)

print(fixed_img.shape)
# print(fixed_img.label)
imshow(fix_img)

network.predict(fixed_img)
```

ì´ ì…€ì˜ ê²°ê³¼ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì œê°€ í…ŒìŠ¤íŠ¸í•  ë•Œ, ì–´ë– í•œ ì‚¬ì§„ì„ ë„£ì–´ë„ ipynb íŒŒì¼ì„ ë³´ì‹œë©´ ë³¼ ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ê°’ $array([[-0.03602253, 0.05814829]])$ ì´ ë‚˜ì™€ì„œ ì•„ ë­”ê°€ ë‹¨ë‹¨íˆ ì˜ëª»ë˜ì—ˆêµ¬ë‚˜ë¥¼ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤ ã…...;

---

## Webcam

```py
labels_dict={0:'Mask',1:'NoMask'}
color_dict={1:(0,0,255),0:(0,255,0)}

size = 4
webcam = cv2.VideoCapture(0) #Use camera 0

# We load the xml file
classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

while True:
    (rval, im) = webcam.read()
    im=cv2.flip(im,1,1) #Flip to act as a mirror

    # Resize the image to speed up detection
    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))

    # detect MultiScale / faces 
    faces = classifier.detectMultiScale(mini)

    # Draw rectangles around each face
    for f in faces:
        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup
        #Save just the rectangle faces in SubRecFaces
        face_img = im[y:y+h, x:x+w]
        resized=cv2.resize(face_img,(224,224))
        normalized=resized/255.0
        reshaped=np.reshape(normalized,(1,3,224,224))
#         print(reshaped.shape)
#         reshaped = np.vstack([reshaped])
#         print(reshaped.shape)
        result=network.predict(reshaped)
        print(result)
        
        label=np.argmax(result,axis=1)[0]
      
        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)
        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)
        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)
        
    # Show the image
    cv2.imshow('LIVE',   im)
    key = cv2.waitKey(100)
    # if Esc key is press then break out of the loop 
    if key == 27: #The Esc key
        break
```
```py
webcam.release()
cv2.destroyAllWindows()
```

ì´ë ‡ê²Œ êµ¬ì„±í•´ë³´ì•˜ìŠµë‹ˆë‹¤! ê°œì¸ì ìœ¼ë¡œ ì•„ì‰¬ì›€ì´ ë§ì´ ë‚¨ëŠ” ì‘í’ˆì´ë¼, í–¥í›„ í‹ˆí‹ˆíˆ ë””ë²„ê¹…ì„ í•´ë³¼ ìƒê°ì…ë‹ˆë‹¤. ë””ë²„ê¹…ì— ì„±ê³µí•œë‹¤ë©´ ì´ ë©˜íŠ¸ ì•„ë˜ì¤„ì— ìˆ˜ì •ì‚¬í•­ë“¤ì„ ì ì–´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤!

ë!