---
title : "Multivariabel Linear Regression"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning_Study
toc : true
toc_sticky: true
comments: true
sidebar_main: true
use_math: true
---
# 딥러닝 공부 4일차

앞서 배운 Linear Regresstion 은 x가 1개인 선형회귀였습니다.
하지만 현실은 어떠한 결과를 내는데 원인이 한가지만 있는경우는 드물죠
만약 x가 여러개면 어떻게될까요?

x가 여러개인 선형회귀를 다항 선형회귀라고 합니다. 퍼셉트론 관점으로는 다중퍼셉트론이라고 할 수 있겠네요.

* 다항 선형 회귀(Multivariabel Linear Regression)
* 가설 함수(Hypothesis Function)
* 평균 제곱 오차(Mean Squared Error)
* 경사하강법(Gradient descent)

## Data Definition

다음과 같은 학슴데이터가 있다고 가정해보겠습니다.
앞서 배운 단순 선형회귀와는 달리 독립변수 x가 3개입니다.
<p align="center"><img src="/MYPICS/lec04/1.png" width = "500" ></p>

가설의 식은 아래와 같게 변합니다.

$$
H(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b
$$

## 파이토치로 구현하기

이러한 다중선형회귀를 파이토치로 구현한다면 다음과 같습니다.

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):
    
    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b

    cost = torch.mean((hypothesis - y_train) ** 2)

    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
        ))
```

코드를 실행한 결과값은 다음과 같습니다.

```py
Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781
Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563634
Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497607
Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435026
Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375730
Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319511
Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222
Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215696
Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167818
Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429
Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079378
```
W1은 약 0.71, W2는 약 0.61, W3는 약 0.68 그리고 b는 0.009 에 가까워지는 것을 볼 수 있습니다. 
cost 값은 초기값 약 3만에서 1로 찾아가는 것을 볼 수 있습니다.

## 선형대수학의 중요성

현실에서 사용되는 딥러닝은 수많은 학습데이터가 존재합니다. 만약에 x의 개수 천개, 만개로 늘어난다면 그에 상응하는 가중치값도 천개, 만개로 늘어날 것이고 그렇다면 위의 파이토치로 구현한 것 처럼 연산을해준다면, 코드가 무한하게 늘어날 것입니다.

이러한 문제점을 **벡터와 행렬연산**으로 바꿔준다면 아주 쉽게 해결됩니다.

<p align="center"><img src="/MYPICS/lec04/2.png" width = "300" ></p>

행렬과 행렬간의 내적을 통해 연산을 간소화시킬 수 있는데요,

$$
H(X) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3}
$$

위와 같은 식을 행렬의 곱으로 표현하면

$$
\left(
    \begin{array}{c}
      x_{1}\ x_{2}\ x_{3}\ \\
    \end{array}
  \right)
\left(
    \begin{array}{c}
      w_{1} \\
      w_{2} \\
      w_{3} \\
    \end{array}
  \right)
\  =
\left(
    \begin{array}{c}
      x_{1}w_{1}+ x_{2}w_{2}+ x_{3}w_{3}\ \\
    \end{array}
  \right)
$$

처럼 간단하게 나타낼 수 있습니다. 또한 두 벡터를 각각 $X$ 와 $W$ 라고한다면 다음과 같이 더욱 간단하게도 표현할 수 있습니다.

$$
H(X) = XW
$$

<p align="center"><img src="/MYPICS/lec04/1.png" width = "500" ></p>

아까위에서 보여드렸던 예제 표입니다. 이렇게 여러가지 변수들을 행렬의 곱으로 표현하면

$$
\left(
    \begin{array}{c}
      x_{11}\ x_{12}\ x_{13}\ \\
      x_{21}\ x_{22}\ x_{23}\ \\
      x_{31}\ x_{32}\ x_{33}\ \\
      x_{41}\ x_{42}\ x_{43}\ \\
      x_{51}\ x_{52}\ x_{53}\ \\
    \end{array}
  \right)
\left(
    \begin{array}{c}
      w_{1} \\
      w_{2} \\
      w_{3} \\
    \end{array}
  \right)
\  =
\left(
    \begin{array}{c}
      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}\ \\
      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}\ \\
      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}\ \\
      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}\ \\
      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}\ \\
    \end{array}
  \right)
  $$

  처럼 표현할 수 있습니다.
  <br>
  <br>

## 행렬의연산을 파이토치로 구현하기

행렬연산을 고려할 것이기 때문에 훈련데이터도 행렬로 선언해주어야합니다.
```py
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  80], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])
```
가중치와 편향을 설정해줍니다
```py
W = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```
여기서 왜 W가 (3x1) 이냐면 x_train이 (5x3) 행렬이기 때문에 행렬의 곱을 하기위한 조건으로 W의 행은 3행이 되고 출력값에 전달하는 값은 하나의 스칼라값이 되어야하기 때문에 1열을 가지게 되는 것입니다.
```py
hypothesis = x_train.matmul(W) + b
```
이 한줄이 강력한 한방입니다. 위의 일반적인 파이토치로 구현하는 연산보다 훨씬 간편해진 것을 알 수 있습니다. 이후에 입력 출력값이 어떻게 변하든 그냥 그 부분만 수정해주면 된다는 점이 아주 맘에드네요.
