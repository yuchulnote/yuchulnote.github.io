---
title : "Lec 10-4 : Resnet, Advance CNN"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, Advance CNN, Resnet]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
published: true
---

# 딥러닝 공부 23일차
<br>
<br>

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/1.png" width = "600" ></p>

CNN을 연구하면서 기존 모델들은 Layer를 깊게 쌓을수록 성능이 좋아질 것이라고 예상했지만 실제로는 20층 이상의 깊이로 갈 수록 오히려 성능이 떨어지는 현상이 존재하였습니다.

그래서 깊이가 깊어질수록 성능이 좋게 만들 수 있는 방법이 없을까해서 나온 방법이 바로 Resnet이라는 방법입니다.

사진과 같이 152층이라는 엄청나게 깊은 네트워크로 사람의 한계를 뛰어넘은 모습을 볼 수 있습니다.

하지만 이렇게 매우 깊은 네트워크에는 문제점이 존재합니다.

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/2.png" width = "600" ></p>

층이 깊어질수록 성능이 떨어지는 것을 보고 직관적으로 **Gradient Vanishing/Explosion** 이 발생할 수 있음을 짐작할 수 있고, 또한 **Degradation** 이라는 층이 깊어지면 오히려 성능이 안좋아지는 현상이 발생함을 알 수 있습니다.

이러한 현상의 해결책으로 optimizer함수를 새로 만들어보고자하는 의견이 있었지만, 새로운 optimizer를 만드는 것은 매우 어렵기 때문에, 새로운 Network를 만들기 시작하게 됩니다.
<br>
<br>

## Residual Block

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/3.png" width = "600" ></p>

기존 네트워크 방식이 $H(x)$ 일 때, 스킵연결을 만들어서 $x$가 더해지는 방식이 residual block 이고 이를 **Skip Connection** 이라고 합니다.

***우리는 F(x)가 최소가 되는 방향이 목표입니다.***

기존 신경망은 $H(x)$가 정답 $y$에 정확히 맵핑이 되는 함수를 찾는 것을 목표로 학습시켜왔던 것이였으므로 기존신경망 $H(x) - x = 0$ 을 만들려 했다면, Resnet은 $H(x)-x=F(x)$로 두어서 $H(x)=x$가 되는 것을 목표로 합니다.

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/4.png" width = "600" ></p>

그래서 이러한 main path 에 skip connection이 연결되는 short cut이 생기게 됩니다.

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/5.png" width = "600" ></p>

결과적으로 Layer가 깊게 쌓여도 에러가 낮고, degradation problem이 발생하지 않는 것을 볼 수 있습니다.

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/6.png" width = "600" ></p>
<br>
<br>

## Deeper Bottleneck Architecture

50층 이상의 깊은 모델을 사용할 때는 연산상의 이점을 위해 "bottleneck" layer (1X1 Convolution)을 이용합니다.

사진에서 왼쪽이 **Basic Block**이고, 오른쪽 그림이 **Bottleneck**입니다.

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/7.png" width = "600" ></p>

### 1X1 Convolution

1 by 1 convolution의 장점은 크게 다음과 같습니다.

> 1. Channel 수 조절
> 2. 연산량 감소
> 3. 비선형성

<img align="center" alt="Convolution with Kernel of size 1x1" src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed_small.gif">

#### 1. Channel 수 조절

채널 수는 우리가 원하는만큼 결정할 수 있습니다. 기존의 합성곱연산에서 채널수가 너무 많게 되면 파라미터 수가 급격히 증가하기 때문에 문제가 생기게 됩니다.

하지만 1X1 Convolution을 사용하면 효율적으로 모델으 구성함과 동시에 만족할만한 성능을 얻을 수 있습니다.

파라미터 수가 급격하게 증가하는 것을 예방하기 때문에, Channel 수를 마음껏 조절할 수 있고, 다양한 크기를 가진 합성곱층을 통해 우리가 원하는 구조의 모델을 구성해볼 수 있습니다.
<br>

#### 2. 계산량 감소

 <p align="center"><img src="/MYPICS/Deep_Learning/lec10-4/8.png" width = "600" ></p>

윗 부분은 1억6천만개의 파라미터 수가 필요하고, 아랏 부분은 4백4십만개가 필요하니 약 4배나 차이나는 것을 보실 수 있습니다.
<br>

#### 3. 비선형성

1X1 Conv를 사용할 때마다 ReLU 함수도 사용을 하게되는데, ReLU 함수를 사용하는 목적 중 하나는 비선형성을 증가시켜줌이 있습니다.

그러므로 1X1 Conv 를 많이 사용할 수록 랠루함수도 많이 사용을하게 되고, 그 말은 즉, 비선형성이 증가하므로 더 복잡한 패턴도 잘 인식할 수 있게 된다는 의미가 됩니다.
<br>
<br>

## Resnet code 구현
d
