---
title : "Lec 09-1: ReLU"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, ReLU]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
---

# 딥러닝 공부 13일차
## ReLU
<br>
<br>

### 시그모이드 함수의 문제점

이제 우리는 새로운 함수인 ReLU함수를 배울 것입니다. 배우기 앞서서 새로움 함수가 필요하다는 것은 기존에 쓰던 함수가 맘에 안드는 부분이 있다는 것이겠죠.

지금까지 잘 사용해왔던 시그모이드 함수의 문제점을 살펴보겠습니다.

<p align="center"><img src="/MYPICS/lec09-1/1.png" width = "600" ></p>

지금까지 우리는 연산과정에서 인풋데이터를 시그모이드함수를 거쳐 출력을 하게 되었었습니다.
그리고 최적의 가중치와 편향을 찾기위해 역전파방법을 사용하게 되었는데요.

이 역전파방법을 수행함에 있어서, 그림에서 시그모이드함수의 파랑색 박스 부분에서는 미분계수를 구하고 역연산하는 과정에서 문제가 발생하지 않습니다.

하지만 빨간박스테투리부근으로 가면 갈 수록, 미분계수가 0에 가까워지는 현상이 발생하게되고 이는 역전파방법을 수행하면서 점점 값이 사라지는, **Vanishing Gradient** 라는 현상이 발생하게 됩니다.

결론적으로 시그모이드 함수를 은닉층에서 사용하는 것은 지양하게 됩니다.

<p align="center"><img src="/MYPICS/lec09-1/2.png" width = "600" ></p>
<br>

**그렇다면 랠루함수는 어떻게 생겼을까요?**

<p align="center"><img src="/MYPICS/lec09-1/3.png" width = "400" ></p>


인공 신경망에서 최고의 인기를 얻고있는 함수라고합니다. 수식은 $f(x)=max(0,x)$ 로 아주 간단합니다.

렐루함수는 0 이하의 음수부터는 0을 출력하고 0부터는 항등함수 x 가 출력되는 함수입니다.

렐루함수의 장점은, 하이퍼블릭탄젠트함수나 시그모이드함수처럼 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠르고, 특정 양수값에 수렴하지 않으므로 깊은 신경망에서 훨씬 더 잘 작동하는 장점이 있습니다.

단점으로는 입력값이 음수이면 기울기가 0이 되어버리기 때문에 연산한 뉴런을 회생하는 것이 매우 어렵습니다. 이러한 문제를 죽은 렐루(dying ReLU)라고 합니다.
