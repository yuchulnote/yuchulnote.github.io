---
title : "Lec 09-1: NonLinear Functions"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, ReLU, Hyperbolic Tangent, Softmax, Leaky ReLU]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
---

# 딥러닝 공부 13일차

<br>
<br>

## ReLU function

### 시그모이드 함수의 문제점

이제 우리는 새로운 함수인 ReLU함수를 배울 것입니다. 배우기 앞서서 새로움 함수가 필요하다는 것은 기존에 쓰던 함수가 맘에 안드는 부분이 있다는 것이겠죠.

지금까지 잘 사용해왔던 시그모이드 함수의 문제점을 살펴보겠습니다.

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-1/1.png" width = "600" ></p>

지금까지 우리는 연산과정에서 인풋데이터를 시그모이드함수를 거쳐 출력을 하게 되었었습니다.
그리고 최적의 가중치와 편향을 찾기위해 역전파방법을 사용하게 되었는데요.

이 역전파방법을 수행함에 있어서, 그림에서 시그모이드함수의 파랑색 박스 부분에서는 미분계수를 구하고 역연산하는 과정에서 문제가 발생하지 않습니다.

하지만 빨간박스테투리부근으로 가면 갈 수록, 미분계수가 0에 가까워지는 현상이 발생하게되고 이는 역전파방법을 수행하면서 점점 값이 사라지는, **Vanishing Gradient** 라는 현상이 발생하게 됩니다.

결론적으로 시그모이드 함수를 은닉층에서 사용하는 것은 지양하게 됩니다.

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-1/2.png" width = "600" ></p>
<br>

또한 시그모이드함수는 원점중심이 아니기 때문에, 출력의 가중치 합이 입력의 가중치 합보다 커질 가능성이 높습니다. 이를 **편향 이동(bias shift)** 라고 합니다.

**그렇다면 랠루함수는 어떻게 생겼을까요?**

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-1/3.png" width = "400" ></p>


인공 신경망에서 최고의 인기를 얻고있는 함수라고합니다. 수식은 $f(x)=max(0,x)$ 로 아주 간단합니다.

렐루함수는 0 이하의 음수부터는 0을 출력하고 0부터는 항등함수 x 가 출력되는 함수입니다.

렐루함수의 장점은, 하이퍼블릭탄젠트함수나 시그모이드함수처럼 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠르고, 특정 양수값에 수렴하지 않으므로 깊은 신경망에서 훨씬 더 잘 작동하는 장점이 있습니다.

단점으로는 입력값이 음수이면 기울기가 0이 되어버리기 때문에 연산한 뉴런을 회생하는 것이 매우 어렵습니다. 이러한 문제를 죽은 렐루(dying ReLU)라고 합니다.

이제 또 다른 비선형함수들을 추가로 알아보겠습니다.
<br>
<br>


## Hyperbolic tangent function(하이퍼볼릭 탄젠트 함수)

하이퍼볼릭 탄젠트 함수는 입력값을 -1과 1사이의 값으로 변환합니다.

생긴 모양은 기존의 탄젠트 함수와 형태가 아주 비슷합니다. 대신 탄젠트함수의 한주기인 $(-\frac{\pi}{2},\frac{\pi}{2})$ 가 $(-1,1)$ 로 바뀌고, 뒤접어져 누워진 형태라고 볼 수 있습니다.

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-1/4.png" width = "400" ></p>

시그모이드 함수는 원점중심이 아니라는 문제점이 있었습니다. 그에비해 $tanh(x)$ 함수는 원점 중심이기 떄문에 편향이동이 발생하지 않는 점이 장점으로 꼽을 수 있겠습니다.

하지만 하이퍼볼릭 탄젠트함수도 시그모이드처럼 함수의 끝부분으로 갈 수록 기울기 소실(Vanishing Gradient)의 문제가 일어날 수 있습니다.
<br>
<br>

## Softmax function

은닉층에서 렐루함수 등을 사용하는 것이 일반적이지만, 그렇다고 앞서 배운 시그모이드나 소프트맥스 함수가 사용되지 않는다는 의미가 이닙니다.

분류 문제를 로지스틱 회귀와 소프트맥스 회귀를 출력층에 적용하여 사용합니다.

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-1/5.png" width = "400" ></p>

소프트맥스 함수는 시그모이드 함수처럼 출력층의 뉴런에서 주로 사용되는데, 시그모이드 함수가 두 가지 선택지 중 하나를 고르는 이진 분류(Binary Classification) 문제에 사용된다면,

세 가지 이상의(상호 배타적인) 선택지 중 하나를 고르는 다중 클래스 분류(MultiClass Classificaiton) 문제에 주로 사용됩니다.
<br>
<br>

## Leaky ReLU function

앞서 배웠던 렐루 함수의 문제점을 보완하기 위해 나온 함수입니다.

죽은 렐루를 보완하기 위한 다양한 렐루함수의 변형 함수들이 있습니다. 그중에 Leaky ReLU만 알아보겠습니다. 리키 렐루 함수는 입력값이 음수일 경우에 0이 아니라 아죽 작은 수를 반환하도록 되어있습니다.

수식은 $f(x)=max(ax,x)$로 아주 간단합니다. 여기서 음수인 부분의 함수의 기울기에 해당하는 a 값을 하이퍼파라미터로 새는 정도를 결정하며, 일반적으로는 0.001 값을 가집니다.

새는 정도를 확실히 보기 위해서
```py
a = 0.1

def leaky_relu(x):
    return np.maximum(a*x, x)

x = np.arange(-5.0, 5.0, 0.1)
y = leaky_relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Leaky ReLU Function')
plt.show()
```
로 함수를 그려보면

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-1/6.png" width = "400" ></p>

위의 그림과 같습니다~!

## 끝!

[참고문헌]

[참고문헌]:https://wikidocs.net/60683
