---
title : "Lec 09-2: Weight Initialization"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, Weight, Weight Initialization]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
---

# 딥러닝 공부 14일차
<br>
<br>

## Weight Initialization(가중치 초기화)

앞서 우리는 여러가지 예제들을 실행해보았습니다. 
분명 같은 모델을 훈련시키는데, 예제에나 유튜브, 책에서 나왔던 수치가 동일하게 뜨지 않는 경우를 보신적이 있으실겁니다.

이는 초기에 랜덤하게 가중치와 편향을 소환하기 때문에, 각자의 노트북에서 모두 다른 결과 값이 나오게 되는 것입니다.

그래서 분명 좋은 학습률이 아니였는데도 불구하고 어떨 때는 손실 함수값이 적게 나오는 경우가 있었습니다. 

다시말해 **가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라집니다.**

*가중치 초기화만 적절히 잘해주면 기울기 소실같은 문제는 방지할 수 있다는 말입니다.*

그렇다면 어떻게 초기화를 진행해야 잘했다고 소문이 날까요?
<br>

* 0으로 초기화해서는 안된다!

조금만 생각해보면 당연한 이야기입니다. 우리는 머신을 학습시킬 때, BackPropagation(역전파) 방법으로, Chain Rule(연쇄 미분)을 통해 Weight(가중치)를 업데이트하게 되는데, 

여기서 가중치값이 0이 되어버린다면 Gradient(기울기) 들이 0으로 다 사라져버리기 때문입니다.
<br>

* Hinton 교수님이 2006년에 Restricted Boltzmann Machine (RBM) 방식을 사용하여 가중치를 초기화해준 후 학습을 시키면 성능이 매우 개선된다는 것을 발표하셨는데요.

더 자세히 알아보겠습니다.
<br>

### Restricted Boltzmann Machine(RBM)

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-2/1.png" width = "600" ></p>

RBM은 같은 층에 있는 놈들은 연결되어있지 않고, 층과 층사이에는 모두 연결되는 구조입니다.
현재는 이 RBM 방식보다 더 간단하고 좋은 방법들이 고안이되어서 잘 사용하지는 않는다고 합니다.

그래도 배우는 입장이니까 어떻게 초기화를 하는지 알아보겠습니다.

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-2/2.png" width = "600" ></p>

맨처음 입력층과 첫 은닉층 사이에서의 Forward 와 Backword 연산을 마치고 나온 가중치들을 고정을 시킵니다.

그 다음 첫번째 은닉층과 두번째 은닉층 사이에서 연산을 해주고 고정
그 다음 그 다음... 이런식으로 층과 층사이 하나씩 Pre-training을 시켜주고나서
출력층까지의 학습을 마치면 RBM 방식을 사용했다고 할 수 잇는 것입니다.
<br>
<br>

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-2/3.png" width = "600" ></p>


### Xavier Initialization(세이비어 초기화)

이 초기화 방법을 고안한 사람의 이름을 따서 세이비어 초기화 혹은 글로럿 초기화라고도 합니다.

이 방법은 균등 분포(Uniform Distribution) 혹은 정규 분포(Normal Distribution)로 초기화 할 때 두 가지 경우로 나뉩니다.

$n_{in}$은 이전층의 뉴런의 개수를, $n_{out}$를 다음층의 뉴런의 개수라고 하면

먼저 균등 분포는 다음과 같습니다.
$$
W \sim Uniform(-\sqrt{\frac{6}{ {n}_{in} + {n}_{out} }}, +\sqrt{\frac{6}{ {n}_{in} + {n}_{out} }})
$$

$\sqrt{\frac{6}{ {n}_{in} + {n}_{out} }}$를 m이라고 하였을 때, -m 과 m 사이의 균등 분포를 의미합니다.

정규 분포로 초기화한 경우는 평균이 0이고, 표준편차가 $\sigma$가 다음을 만족하도록 합니다.

$$
σ=\sqrt{\frac { 2 }{ { n }_{ in }+{ n }_{ out } } }
$$

정규분포는 $X \sim N(0,\sigma^2)$ 으로 표현합니다.

세이비어 초기화는 특정 층이 너무 주목을 받거나 뒤쳐지는 것을 방지합니다.
그리고 세이비어 초기화는 시그모이드함수나 하이퍼볼릭탄젠트함수와 같은 S자 형태인 활성화 함수와 사용할 때는 좋은 성능을 보이지만, ReLU와 함께 사용할 때는 성능이 좋지 않습니다.

ReLU함수와 잘 어울리는 것은 He 초기화 방법이 있습니다.
<br>
<br>

### He Initialization

He 초기화방법은 세이비어 방법과 다르게 다음층의 뉴런의 개수를 고려하지 않습니다.

균등 분포로 고려할 때는 다음과 같습니다.

$$
W\sim Uniform(- \sqrt{\frac { 6 }{ { n }_{ in } } } , \space\space + \sqrt{\frac { 6 }{ { n }_{ in } } } )
$$

정규 분포로 초기화할 경우에는 표준 편차 $\sigma$ 가 다음을 만족하도록 합니다.

$$
σ=\sqrt{\frac { 2 }{ { n }_{ in } } }
$$

## 끝!

[참고문헌]

[참고문헌]:https://wikidocs.net/61271
