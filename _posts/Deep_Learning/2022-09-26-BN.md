---
title : "BatchNormalization êµ¬í˜„í•˜ê¸°(with Python Numpy)"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning, Numpy, Python, BatchNormalization]
toc : true
toc_sticky: true
comments: true
sidebar_main: true
use_math: true
published: true
---

>ì˜¤ëŠ˜ì€ BatchNormalizationì„ íŒŒì´ì¬ ë„˜íŒŒì´ë¡œ êµ¬í˜„í•˜ê³  í•˜ë‚˜ì”© ëœ¯ì–´ì„œ ê³µë¶€í•´ë³´ë ¤í•©ë‹ˆë‹¤.

[Batch Normalization ë³µìŠµí•˜ê¸°] ğŸ‘ˆ  í´ë¦­!

[Batch Normalization ë³µìŠµí•˜ê¸°]:https://yuchulnote.github.io/deep_learning_study/Lab09-3,4/#batch-normalization

```py
class BatchNormalization:

    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None # í•©ì„±ê³±ê³„ì¸µì€ 4ì°¨ì›, ì™„ì „ì—°ê²° ê³„ì¸µì€ 2ì°¨ì›

        # ì‹œí—˜í•  ë•Œ ì‚¬ìš©í•  í‰ê· ê³¼ ë¶„ì‚°
        self.running_mean = running_mean
        self.running_var = running_var

        # backwardì‹œ ì‚¬ìš©í•  ì¤‘ê°„ ë°ì´í„°
        self.batch_size = None
        self.xc = None # í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ...
        self.std = None #í‘œì¤€í¸ì°¨
        self.dgamma = None #ê°ë§ˆë¯¸ë¶„ê°’
        self.dbeta = None #ë² íƒ€ë¯¸ë¶„ê°’

    def forward(self, x, train_flg=True): #ìˆœì „íŒŒ, í•™ìŠµë°ì´í„° í•™ìŠµì‹œ
        self.input_shape = x.shape #ì…ë ¥ ë°ì´í„° ëª¨ì–‘
        if x.ndim != 2: #ì…ë ¥ ì°¨ì›ê°’ì´ 2ê°€ ì•„ë‹ˆë¼ë©´, í•©ì„±ê³±ê³„ì¸µ
            N, C, H, W = x.shape # ìˆœì„œëŒ€ë¡œ BatchSize, Channelìˆ˜, ë†’ì´, ë„ˆë¹„
            x = x.reshape(N, -1) # Batchsizeí¬ê¸°ë§Œí¼ì„ í–‰ìœ¼ë¡œ ê°€ì§€ëŠ” í–‰ë ¬ë¡œ resize
        
        out = self.__forward(x, trainflg) #outê°’ì— BN ìˆœì „íŒŒ ê°’ì„ ì €ì¥

        return out.reshape(*self.input_shape) # *(asterisk), ì…ë ¥ëª¨ì–‘ì„ ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì‚¬ìš©! ì–´ë–¤ ì…ë ¥ëª¨ì–‘ì´ ë“¤ì–´ì™€ë„ ë™ì‘í•˜ê²Œë”

    def __forward(self, x, train_flg):
        if self.running_mean is None:
            N, D = x.shape # N: batch_size, D: Features
            # ê° feature ë³„ë¡œ ì •ê·œí™”ë¥¼ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— í‰ê· ê³¼ ë¶„ì‚°ì„ feature ê°œìˆ˜ë§Œí¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)
        
        if train_flg:
            mu = x.mean(axis=0) #mu : í‰ê· 
            xc = x - mu # í‰ê· ì´ 0ì´ ë˜ë„ë¡ xì—ì„œ í‰ê· ë§Œí¼ ë¹¼ê¸°
            var = np.mean(xc**2, axis=0)
            std = np.sqrt(var + 10e-7)
            xn = xc/std

            self.batch_size = x.shape[0]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu
            self.running_var = self.momentum * self.running_var
        else:
            xc = x - self.running_mean
            xn = xc / ((np.sqrt(self.running_var + 10e-7)))

        out = self.gamma * xn + self.beta
        return out
    
    def backward(self, dout):
        if dout.ndim != 2:
            N, C, H, W = dout.shpae
            dout = dout.reshape(N, -1)
        
        dx = self.__backward(dout)

        dx = dx.reshape(*self.input_shape)
        return dx
    
    def __backward(self, dout):
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xn * dout, axis=0)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)
        dvar = 0.5 * dstd / self.std
        dxc += (2.0 / self.batch_size) * self.xc * dvar
        dmu = np.sum(dxc, axis=0)
        dx = dxc - dmu / self.batch_size

        self.dgamma = dgamma
        self.dbeta = dbeta

        return dx
```
