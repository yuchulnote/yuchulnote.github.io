---
title : "Lec 08-2: MultiLayer Perceptron(MLP) & Backpropagation"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, MLP, Perceptron, MultiLayer Perceptron]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
---

# 딥러닝 공부 12일차
## BackPropagation(역전파)
<br>
<br>

인공 신경망을 열심히 만들고 돌렸는데, 예측값과 실제값이 다를 경우에 우리는 입력값을 다시 조정하여서 적절한 W 와 b 값을 다시 찾아야합니다.

하지만 이런 과정은 결코 쉽지않았고 과거의 DNN의 큰 문제로 남겨져 있었습니다.

하지만 현재, 이러한 문제는 BackPropagation, 역전파라는 방법으로 해결할 수 있습니다.
<br>

### 인공 신경망의 이해(Neural Network Overview)

<p align="center"><img src="/MYPICS/lec08-2/1.png" width = "350" ></p>

예제를 위해 사용될 인공신경망입니다. 입력층이 2개, 은닉층이 2개, 출력층이 2개입니다.

z는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들의 합을 의미합니다. 이 값은 아직 활성화함수를 거치기 전 상태입니다.

이제 h값을 통해 활성화함수를 거치게 됩니다. 우리는 활성화함수로 시그모이드함수를 사용할 것 입니다.

o 값은 출력값을 의미합니다.
<br>

### 순전파(Forward Propagation)

<p align="center"><img src="/MYPICS/lec08-2/2.png" width = "400" ></p>

주어진 값이 위 그림과 같을 때, 순전파를 진행해보겠습니다. 소수점 앞의 0은 생략된 사진입니다.

$z_{1}$ 과 $z_{2}$ 는 입력층의 입력값들을 받습니다.

$$
z_{1}=W_{1}x_{1} + W_{2}x_{2}=0.3 \text{×} 0.1 + 0.25 \text{×} 0.2= 0.08
$$

$$
z_{2}=W_{3}x_{1} + W_{4}x_{2}=0.4 \text{×} 0.1 + 0.35 \text{×} 0.2= 0.11
$$

이제 각각의 $z_{1}$ 과 $z_{2}$ 는 은닉층의 활성화함수를 거치게 됩니다.

$$
h_{1}=sigmoid(z_{1}) = 0.51998934
$$

$$
h_{2}=sigmoid(z_{2}) = 0.52747230
$$

이제 $h_{1}$ 과 $h_{2}$ 는 이제 출력층의 뉴런으로 향하게 되는데, 다시 각각의 가중치와 곱해져서 전달이 되게 됩니다.

$$
z_{3}=W_{5}h_{1}+W_{6}h_{2} = 0.45 \text{×} h_{1} + 0.4 \text{×} h_{2} = 0.44498412
$$

$$
z_{4}=W_{7}h_{1}+W_{8}h_{2} = 0.7 \text{×} h_{1} + 0.6 \text{×} h_{2} = 0.68047592
$$

$z_{3}$ 와 $z_{4}$ 는 출력층 뉴런에서 시그모이드 함수를 지나게되고, 이 값들은 최종적으로 계산된 출력값입니다. 즉, 예측값입니다.

$$
o_{1}=sigmoid(z_{3})=0.60944600
$$

$$
o_{2}=sigmoid(z_{4})=0.66384491
$$

이제 필요한 것은, 예측갑과 실제값의 오차를 계산하기 위한 오차함수, 즉 손실함수를 선택하는 것입니다.

여기서는 MSE 방식을 사용하도록 하겠습니다. 예측값을 output, 실제값을 target이라고 표현하겠습니다.

$$
E_{o1}=\frac{1}{2}(target_{o1}-output_{o1})^{2}=0.02193381
$$

$$
E_{o2}=\frac{1}{2}(target_{o2}-output_{o2})^{2}=0.00203809
$$

$$
E_{total}=E_{o1}+E_{o2}=0.02397190
$$

