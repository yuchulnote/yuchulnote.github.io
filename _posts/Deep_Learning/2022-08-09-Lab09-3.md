---
title : "Lec 09-3: Dropout"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, Dropout]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
---

# 딥러닝 공부 15일차
<br>
<br>

이전에 Overfitting(과적합)을 방지할 수 있는 방법을 소개했었습니다.
다시 몇가지 적어보면

* More training data
* Reduce the number of features
* Regularization
* **Dropout**

과적합을 방지하는 방법 중 하나인 Dropout에 대해서 오늘 공부해보겠습니다.

## Dropout

dropout은 쉽게말하면 매 층마다 어떠한 확률로 일부는 버리고 남은 나머지들로 학습해나가는 것을 말합니다.

즉, 신경망의 일부를 사용하지 않는 방법입니다.

<p align="center"><img src="/MYPICS/Deep_Learning/lec09-3/1.png" width = "600" ></p>

드롭아웃은 신경망 학습시에만 사용하고, 예측시에는 사용하지 않습니다.

학습시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 과적합을 방지해줍니다.

코드로는 다음과 같이 사용할 수 있습니다.
```py
...

# nn layers
linear1 = torch.nn.Linear(784, 512, bias=True)
linear2 = torch.nn.Linear(512, 512, bias=True)
linear3 = torch.nn.Linear(512, 512, bias=True)
linear4 = torch.nn.Linear(512, 512, bias=True)
linear5 = torch.nn.Linear(512, 10, bias=True)
relu = torch.nn.ReLU()
dropout = torch.nn.Dropout(p=drop_prob)

# model
model = torch.nn.Sequential(linear1, relu, dropout,
 linear2, relu, dropout,
 linear3, relu, dropout,
 linear4, relu, dropout,
 linear5).to(device)
```

## 끝!

[참고문헌]

[참고문헌]:https://wikidocs.net/60751