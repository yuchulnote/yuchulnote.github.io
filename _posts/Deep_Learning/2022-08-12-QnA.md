---
title : "Deep Learning code 구현을 위한 Q&A"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, Q&A]
toc : true
toc_sticky: true 
comments: true
sidebar_main: true
use_math: true
published: true
---

# 딥러닝 공부 18일차
<br>
<br>

>그 동안 딥러닝 공부를 하면서 들었던 의문점들을 총 집합해서 공부하고 알아보겠습니다.
>제가 들었던 의문이면 다른 사람들도 똑같이 의문을 가지지 않았을까 싶어 따로 정리해보려합니다.
>도움이 되었으면 좋겠습니다~

## Parameter VS Hiperparameter

파라미터는 한국어로 매개변수입니다. 파라미터는 모델 내부에서 결정되는 변수입니다. 또한 그 값은 데이터로부터 결정됩니다. 

하이퍼 파라미터는 모델링할 때 사용자가 직접 세팅해주는 값을 뜻합니다.
learning rate나 서포트 벡터 머신에서의 C, sigma 값, KNN에서의 K값 등등 굉장히 많습니다.
머신러닝 모델을 쓸 때 사용자가 직접 세팅해야 하는 값은 상당히 많습니다. 

그 모든 게 다 하이퍼 파라미터입니다. 하지만, 많은 사람들이 그런 값들을 조정할 때 그냥 '모델의 파라미터를 조정한다'라는 표현을 씁니다. 원칙적으로는 '모델의 하이퍼 파라미터를 조정한다'라고 해야 합니다.

**파라미터와 하이퍼 파라미터를 구분하는 기준은 사용자가 직접 설정하느냐 아니냐입니다. 사용자가 직접 설정하면 하이퍼 파라미터, 모델 혹은 데이터에 의해 결정되면 파라미터입니다.**
<br>

## Conv1d, 2d, 3d 차이

말 그대로다. 1차원 배열 데이터에는 Conv1D를, 2차원 배열 데이터에는 Conv2D를 사용한다. 3차원 배열이면 Conv3D를 사용한다.

즉, Conv1D, Conv2D, Conv3D 차이는 입력 데이터의 차원이다. 그런데 여기서 끝나면 아쉬우니 코드로 더 살펴봅시다.

필자는 아직 파이토치밖에 공부를 안했는데 찾아본 예시 코드가 텐서플로우입니다... 하지만 이해는 할 수 있으니 갖다 써보겠습니다.

```py
model = tf.keras.models.Sequential([
    # Note the input shape is the desired size of the image 150x150 with 3 bytes color
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
```

위 코드를 보면 입력 차원(input_shape)이 3차원인데 왜 Conv2D지? 라는 의문이 생길 수 있고, 또

```py
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

2번째 줄이랑 3번째 줄만 보명. 입력 차원이 (tokenizer.vocab_size, 64)로 2차원인데 Conv1D를 쓰지....? 라는 의문이 든다.

그래서 궁극적으로 **Conv(n)d** 의 차이점은 **합성곱을 진행할 입력 데이터의 차원** 을 의미합니다. 

합성곱 진행 방향을 고려해야 한다는 말입니다.

