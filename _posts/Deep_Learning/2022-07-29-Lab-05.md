---
title : "Lec 05: Logistic Regression"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, Regression]
toc : true
toc_sticky: true
comments: true
sidebar_main: true
use_math: true
---

# 딥러닝 공부 6일차
## Logistic Regression
<br>
<br>

일상속의 많은 문제들중에 두개의 선택지중에 정답을 고르는 문제들 같은 경우가 많습니다.
예를들어 계절학기의 Pass or Non_pass 라던지, 받은 메일이 스팸인지 아닌지 이런식으로 이분법적으로 나누는 경우를 **Binary Classification(이진 분류)** 라고 합니다.

그리고 이러한 이진 분류를 풀기 위한 대표적인 알고리즘이 **Logistic Regression** 입니다.

이 로지스틱 회귀는 이진 분류의 특성 때문에 Regression(회귀)로 사용하지만 Classification(분류)로도 사용이 가능합니다.
<br>
<br>

### Binary Classification

이번 여름방학 계절학기 학생들의 성적으로 패논패를 부여한다고 해봅시다.
50점 이상이면 패스, 50점 아래면 논패스라고 예를 들어보겠습니다.

<p align="center"><img src="/MYPICS/lec05/1.png" width = "200" ></p>

위와 같이 6명의 학생의 점수가 있다고 생각해봅시다. 6명의 학생의 점수중 앞서 말한 것 처럼 패스한 학생을 1 논패스한 학생을 0으로 표햔해보면

<p align="center"><img src="/MYPICS/lec05/2.png" width = "500" ></p>

그 동안 배웠던 가설함수인 직선의 형태로는 위 그림과 같은 모양을 표현하기엔 다소 어려움이 있어보입니다.

그래서 우리는 이번에 새로운 함수를 배우게됩니다.
이번 로지스틱 회귀에서의 가설은 $H(x) = Wx + b$ 가 아니라 $H(x) = f(Wx + b)$ 꼴을 사용하게 됩니다.

위의 그림과 같은 함수를 표현하기위한 함수로 시그모이드함수가 있습니다.
<br>
<br>

### 시그모이드 함수

시그모이드 함수의 원형은 아래와 같습니다. 이제 x대신에 Wx+b 가 대입되면 됩니다.

$$
sigmoid(x) = \frac{1}{1 + e^{-x}}
$$

시그모이드 함수의 그래프를 그려보면 다음과 같습니다. 
<p align="center"><img src="/MYPICS/lec05/3.png" width = "600" ></p>

위 그림이 시그모이드 함수입니다. x가 $-\infty$ 로 향하면 0으로 다가가고, 반대로 $\infty$ 로 향하면 1로 다가가는 함수입니다. 추가로 $f(0)=0.5$ 입니다.

$$
H(x) = sigmoid(Wx + b) = \frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)
$$

선형회귀에서는 W가 기울기를 b가 y절편을 의미했다면, 시그모이드함수에서 W 와 b는 함수의 모양과 위치의 차이가 변화합니다.

직접 코드를 쳐보면서 함수가 어떻게 변하는지 확인해봅시다.

* 파이썬에서는 그래프를 그릴 수 있는 도구로서 Matplotlib을 사용할 수 있습니다.
* [Matplotlib 자세한 사용법] 은 여기를 참고해주세요.

[Matplotlib 자세한 사용법]:https://todayisbetterthanyesterday.tistory.com/67

```py
%matplotlib inline
import numpy as np # 넘파이 사용
import matplotlib.pyplot as plt # 맷플롯립사용
```

만약 위의 코드를 주피터 노트북에 실행했더니 오류가 나시는 분들은 matplotlib가 안깔려 있어서 그렇습니다. 그럴 때는 아래 코드를 주피터노트북에서 실행해주세요.
```py
pip install matplotlib
```
설치가 완료되셨으면 직접 코드를 입력하면서 함수의 변화를 관찰하시면 좋을 것 같습니다.

시그모이드 함수를 불러오려면 다음과 같이 코드를 입력하시면 됩니다.
```py
def sigmoid(x): # 시그모이드 함수 정의
    return 1/(1+np.exp(-x))
```
np.exp라고하면 자연상수를 밑으로 가지는 지수함수가 소환이됩니다 그다음 괄호에 들어오는 수 또는 문자가 지수의 역할을 합니다.

```py
x = np.arange(-5.0, 5.0, 0.1) # x갑인 -5부터 5까지 0.1칸마다 점을찍겠다
# -> 직선은 점들의 집합임을 생각하면 좋을 것 같습니다.
y = sigmoid(x)

plt.plot(x, y, 'g') # 초록색 선으로 그려라라는 의미
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```
<p align="center"><img src="/MYPICS/lec05/4.png" width = "400" ></p>

위의 그래프 사진과 동일하죠? 그렇다면 잘 소환한 것입니다.
<br>

#### W 값에 따른 경사도 변화

이제 W값을 변화시켰을 때, 함수가 어떻게 변화하는지 알아보겠습니다.

시그모이드 함수로 가설을 사용할때는 $H(x) = f(Wx + b)$ 라고 앞에서 설명드렸습니다.

하지만 여기서는 W 값에따른 변화만을 관찰하기위해서 $f(Wx)$ 즉, 편향을 제외하고 코드를 입력해보았습니다.

```py
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(0.5*x) # W=0.5
y2 = sigmoid(x) # W=1
y3 = sigmoid(2*x) # W=2

plt.plot(x, y1, 'r--') # W의 값이 0.5일때, 빨간 점선
plt.plot(x, y2, 'g') # W의 값이 1일때, 초록선
plt.plot(x, y3, 'b--') # W의 값이 2일때, 파랑 점선
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```
<p align="center"><img src="/MYPICS/lec05/5.png" width = "400" ></p>

이와같이 가중치 값이 크면 경사가 커지고, 가중치값이 작아지면 경사가 작아지는 것을 확인하실 수 있습니다.

간단히 생각해보면, 극단적으로 시그모이드함수에서 가중치값이 0이라고 생각해보면

$$
Sigmoid(x) = \frac{1}{1+e^{0}} = \frac{1}{2}
$$

즉 $y=0.5$ 인 경사각이 없는 직선이 나온다고 생각해볼 수도 있을 것 같습니다.
<br>

#### b 값에 따른 좌,우 이동

이제 b 값에 따라서 그래프가 어떻게 달라지는지 확인해보겠습니다.

위에서 함수를 설명해드렸으니 감이 오시죠? 이번엔 $f(x+b)$ 꼴로 W를 1로 고정해놓고 편향이 추가된 모습입니다.

<p align="center"><img src="/MYPICS/lec05/6.png" width = "300" ></p>

지정해야할 색깔이 많을 때는 'C~' 를 이용하면 편할 것 같아요~

```py
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(x+0.5)
y2 = sigmoid(x+1)
y3 = sigmoid(x+1.5)
y4 = sigmoid(x)
y5 = sigmoid(x-0.5)
y6 = sigmoid(x-1)
y7 = sigmoid(x-1.5)

plt.plot(x, y1, 'C0--') # x + 0.5
plt.plot(x, y2, 'C1--') # x + 1
plt.plot(x, y3, 'C2--') # x + 1.5
plt.plot(x, y4, 'k') #원래 시그모이드함수 
plt.plot(x, y5, 'C3--') # x - 0.5
plt.plot(x, y6, 'C4--') # x - 1
plt.plot(x, y7, 'C5--') # x - 1.5

plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```
<p align="center"><img src="/MYPICS/lec05/7.png" width = "400" ></p>

편향이 클수록 함수가 좌측으로 평행이동 되고 있습니다.
또한 편향이 작을수록 우측으로 평행이동 되는 모습입니다.

이차함수의 꼭지점의 평행이동을 생각해보면 이해가 더 빠를 것 같습니다.
<br>
<br>

### Cost Function(비용 함수)

이제 로지스틱 회귀의 가설함수가 $H(x) = sigmoid(Wx + b)$ 인 것은 알았습니다.
이제 이 가설함수로 비용 함수를 설정할건데,

선형 회귀에서 배웠던 MSE 함수로 하면 될까요?

$$
cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2
$$

기존의 선형회귀에서는 전구간에서 아래로 볼록했기 때문에 최적의 W 와 b를 찾는 과정이 단순 미분을 통해 기울어진 방향으로 어떻게든 이동하게 되었지만 (아래 그림참고)

<p align="center"><img src="/MYPICS/lec05/9.png" width = "500" ></p>

[사진출처]

[사진출처]:http://taewan.kim/post/cost_function_derivation/

이제는 위의 비용함수 수식에서 가설은 이제 직선이 아니라 시그모이드함수가 대입을하게 됩니다.

<p align="center"><img src="/MYPICS/lec05/8.png" width = "300" ></p>

<p align="center"><img src="/MYPICS/lec05/10.png" width = "600" ></p>

참고: [모두를 위한 딥러닝-sung kim]

[모두를 위한 딥러닝-sung kim]:https://www.youtube.com/watch?v=TxIVr-nk1so&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=7

위 그림처럼 무작정 MSE 함수에 시그모이드 함수를 때려박고 미분을 하게되면 local minimum, 즉 지역국소값을 모델이 학습할때 최소값이라고 착각하게 될 수도 있기 때문에 좋지 않습니다.

우리는 전체 함수에서의 최소값, Global Minimum을 찾는 것이 목표이기 때문입니다.

시그모이드 함수의 특징으로 Classification을 할 수 있다고 소개해드렸었습니다.

보통 $f(0)$ 인 0.5를 임계값으로 설정하고 넘어가면 1(True)을 출력하고 넘지 못하면 0(False)를 출력한다고 했었습니다.

이를 조금 더 확장해서 생각해보면 만약 실제값이 1이고 예측값이 0근처라면 오차가 커야겠지요?
반대로 실제값이 0이고 예측값이 1쪽이면 오차가 커야할겁니다.

이를 만족시킬 수 있는 함수로 로그함수를 이용할 수 있습니다.

<p align="center"><img src="/MYPICS/lec05/11.png" width = "600" ></p>

그래서 오차를 표현하기위해 실제값이 1일 때의 그래프가 $-log(x)$, 빨간그래프.

실제값이 0일 때의 그래프가 $-log(1-x)$,  파랑그래프로 나타낼 수 있습니다.
<br>

$$
\text{if } y=1 → \text{cost}\left( H(x), y \right) = -\log(H(x))
$$

$$
\text{if } y=0 → \text{cost}\left( H(x), y \right) = -\log(1-H(x))
$$

실제값이 1이라고 했을 때, 예측값도 1이라면 $-log1=0$ 이기때문에 완벽한 손실함수의 최소값이 나옵니다.
혹은 예측값이 0이라고한다면, $-log0=-\infty$ 로 손실이 최대가 납니다.

반대로 실제값이 0이라고 했을 때, 예측값이 0이면 $-log(1-0)=0$ 으로 완벽한 예측을 했다고 볼 수 있고, 예측을 1이라고 했다면, $-log(1-1)=-\infty$ 으로 손실이 최대가 나오게 됩니다.

**$-\infty$이면 0일때 손실이 최소가 아니라 -무한대니까 더 최소아니야??**

라고 생각하실 수 있습니다. 

**여기서 -의 의미는 실제로 음수의 개념이 아니라, 손실함수를 표현하기위해 로그함수를 뒤집어주기위한 -이므로 실제로 손실은 $\infty$ 라고 보는 것이 합당합니다.**

그래서 저희는 이제 두 경우의 수를 모두 합치면 다음과 같다고 할 수 있습니다.

$$
\text{cost}\left( H(x), y \right) = -[ylogH(x) + (1-y)log(1-H(x))]
$$

왜 두개의 식이 합쳐졌을까요?? 

이유는 H(x) 값에 0 또는 1 밖에 들어가지 않으므로 앞에서 설명했던 것 처럼 실제값이 1이라면 덧셈을 기준으로 우측항이 실제값이 0이라면 덧셈을 기준으로 좌측항이 사라지기 때문입니다.

이제 모든 오차의 평균을 구하면 손실함수는 아래와 같습니다.

$$
cost(W) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]
$$

이제 위 비용 함수를 이용하여 경사 하강법을 사용하면 최적의 W 와 b 값을 찾을 수 있습니다.

$$
W := W - α\frac{∂}{∂W}cost(W)
$$
