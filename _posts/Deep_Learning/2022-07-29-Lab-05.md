---
title : "Lec 05: Logistic Regression"
categories :
    - Deep_Learning_Study
tag :
    - [Deep_Learning_Study, Regression]
toc : true
toc_sticky: true
comments: true
sidebar_main: true
use_math: true
---

# 딥러닝 공부 6일차
## Logistic Regression
<br>
<br>

일상속의 많은 문제들중에 두개의 선택지중에 정답을 고르는 문제들 같은 경우가 많습니다.
예를들어 계절학기의 Pass or Non_pass 라던지, 받은 메일이 스팸인지 아닌지 이런식으로 이분법적으로 나누는 경우를 **Binary Classification(이진 분류)** 라고 합니다.

그리고 이러한 이진 분류를 풀기 위한 대표적인 알고리즘이 **Logistic Regression** 입니다.

이 로지스틱 회귀는 이진 분류의 특성 때문에 Regression(회귀)로 사용하지만 Classification(분류)로도 사용이 가능합니다.
<br>
<br>

### Binary Classification

이번 여름방학 계절학기 학생들의 성적으로 패논패를 부여한다고 해봅시다.
50점 이상이면 패스, 50점 아래면 논패스라고 예를 들어보겠습니다.

<p align="center"><img src="/MYPICS/lec05/1.png" width = "200" ></p>

위와 같이 6명의 학생의 점수가 있다고 생각해봅시다. 6명의 학생의 점수중 앞서 말한 것 처럼 패스한 학생을 1 논패스한 학생을 0으로 표햔해보면

<p align="center"><img src="/MYPICS/lec05/2.png" width = "500" ></p>

그 동안 배웠던 가설함수인 직선의 형태로는 위 그림과 같은 모양을 표현하기엔 다소 어려움이 있어보입니다.

그래서 우리는 이번에 새로운 함수를 배우게됩니다.
이번 로지스틱 회귀에서의 가설은 $H(x) = Wx + b$ 가 아니라 $H(x) = f(Wx + b)$ 꼴을 사용하게 됩니다.

위의 그림과 같은 함수를 표현하기위한 함수로 시그모이드함수가 있습니다.
<br>
<br>

### 시그모이드 함수

시그모이드 함수의 원형은 아래와 같습니다. 이제 x대신에 Wx+b 가 대입되면 됩니다.
$$
sigmoid(x) = \frac{1}{1 + e^{-x}}
$$
시그모이드 함수의 그래프를 그려보면 다음과 같습니다. 
<p align="center"><img src="/MYPICS/lec05/3.png" width = "600" ></p>

$$
H(x) = sigmoid(Wx + b) = \frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)
$$

선형회귀에서는 W가 기울기를 b가 y절편을 의미했다면, 시그모이드함수에서 W 와 b는 함수의 모양과 위치의 차이가 변화합니다.

직접 코드를 쳐보면서 함수가 어떻게 변하는지 확인해봅시다.

* 파이썬에서는 그래프를 그릴 수 있는 도구로서 Matplotlib을 사용할 수 있습니다.

```py
%matplotlib inline
import numpy as np # 넘파이 사용
import matplotlib.pyplot as plt # 맷플롯립사용
```