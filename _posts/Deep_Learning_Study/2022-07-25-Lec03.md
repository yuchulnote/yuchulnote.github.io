---
title : "Linear Regression"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning_Study
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---
# 딥러닝 공부 3일차

* Data definition, 데이터 수집
* Hypothesis, 가설 수립
* Compute Loss, 손실계산
* Gradient Descent, 경사하강법
<br>
<br>
## Data Definition

훈련 데이터셋과 테스트 데이터셋
<p align="center"><img src="/MYPICS/lec03/1.png" width = "500" ></p>
공부한 시간에 비례해서 점수가 나온다고 했을 때 4시간 공부하면 몇점을 얻을 수 있는가? 에대한 데이터입니다.

<br> 훈련 데이터셋은 입력과 출력으로 나뉘는데 이는 파이토치의 텐서의 형태로 이루어져있어야합니다. 

<br> 여기서 x_train은 공부한시간 y_train은 점수입니다.
```py
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [5], [9]])
```
저는 교재 예제에서 숫자를 조금 바꿔서 응용해보겠습니다.
<br>
<br>

## Hypothesis 수립
선형회귀에서 가설은 직선의형태이고 그 중 학습데이터와 가장 유사한 직선을 찾아야합니다.
$$y= Wx+b$$
가설의 H를 따서 사용하기도 합니다.
$$H(x)= Wx+b$$ 

**여기서 x와 곱해지는 W는 가중치(weight)를 의미하고, b는 편향(bias)를 의미합니다.**
가중치 W는 기울기를 의미하고 b는 y절편을 의미하겠죠? 배운거 어디 안갑니다~
<br>
<br>

## Cost Function
앞으로 딥러닝을 학습하면서 비용 함수(cost function) = 손실 함수(loss function) = 오차함수(error function) = 목적 함수(objective fuction)

비용함수에 대한 이해를 위한 예제입니다. 이 비용함수는 선형대수학의 최소제곱해와 같습니다. (이하 최소제곱직선)
이 예제를 분석해보면 왜 선형대수학에서 최소'제곱'직선 이라고 불리는지 알 수 있습니다.

<p align="center"><img src="/MYPICS/lec03/3.png" width = "500" ></p>

그래프상에 한직선으로 표현할 수 없는 네개의 점이 있다고 합시다.

<p align="center"><img src="/MYPICS/lec03/4.png" width = "500" ></p>

최대한 4개의 점과 가까운 직선을 찾아주세요 하는 문제인건데, 사람에따라 빨간선이 혹은 검정선이 제일 가깝다고 생각 할 수 있지만 이를 수학적으로

여기서 저장을하구